{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee17909c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(<class 'toile.schema.SliceRecordingFrame'>, <class 'toile.schema.ImageSample'>): <atdata.lens.Lens object at 0x10f889c40>}\n"
     ]
    }
   ],
   "source": [
    "import toile.schema as ts\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a9c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ts.SliceRecordingFrame(\n",
    "    data = np.random.randn( 256, 256 ),\n",
    "    mouse_id = 'test_mouse',\n",
    "    slice_id = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dc8248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webdataset as wds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ddeb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path( 'output' )\n",
    "wds_stem = 'test-frames'\n",
    "\n",
    "k_test = 1000\n",
    "\n",
    "## Write sharded dataset\n",
    "\n",
    "output_dir.mkdir( parents = True, exist_ok = True )\n",
    "\n",
    "file_pattern = (\n",
    "    output_dir\n",
    "    / (f'{wds_stem}' + '-{shard_id}.tar.gz')\n",
    ").as_posix()\n",
    "file_wds_pattern = file_pattern.format( shard_id = '%06d' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dece602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing output/test-frames-000000.tar.gz 0 0.0 GB 0\n",
      "# writing output/test-frames-000001.tar.gz 73 0.0 GB 73\n",
      "# writing output/test-frames-000002.tar.gz 73 0.0 GB 146\n",
      "# writing output/test-frames-000003.tar.gz 73 0.0 GB 219\n",
      "# writing output/test-frames-000004.tar.gz 73 0.0 GB 292\n",
      "# writing output/test-frames-000005.tar.gz 73 0.0 GB 365\n",
      "# writing output/test-frames-000006.tar.gz 73 0.0 GB 438\n",
      "# writing output/test-frames-000007.tar.gz 73 0.0 GB 511\n",
      "# writing output/test-frames-000008.tar.gz 73 0.0 GB 584\n",
      "# writing output/test-frames-000009.tar.gz 73 0.0 GB 657\n",
      "# writing output/test-frames-000010.tar.gz 73 0.0 GB 730\n",
      "# writing output/test-frames-000011.tar.gz 73 0.0 GB 803\n",
      "# writing output/test-frames-000012.tar.gz 73 0.0 GB 876\n",
      "# writing output/test-frames-000013.tar.gz 73 0.0 GB 949\n"
     ]
    }
   ],
   "source": [
    "shard_size_mb = 38\n",
    "# TODO dunno why this works\n",
    "shard_maxsize = shard_size_mb * (10 ** 6)\n",
    "\n",
    "with wds.ShardWriter(\n",
    "    pattern = file_wds_pattern,\n",
    "    maxsize = shard_maxsize,\n",
    ") as sink:\n",
    "    \n",
    "    for i_sample in range( k_test ):\n",
    "\n",
    "        new_sample = ts.SliceRecordingFrame(\n",
    "            data = np.random.randn( 256, 256 ),\n",
    "            mouse_id = 'test_mouse',\n",
    "            slice_id = 1,\n",
    "        )\n",
    "\n",
    "        sink.write( new_sample.as_wds )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc4d826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import atdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c00f1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = file_pattern.format( shard_id = '{000000..000014}' )\n",
    "ds = atdata.Dataset[ts.SliceRecordingFrame]( dataset_url )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7423d817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<atdata.dataset.SampleBatch object at 0x10c977980>\n"
     ]
    }
   ],
   "source": [
    "batch: atdata.SampleBatch[ts.SliceRecordingFrame]\n",
    "for batch in ds.shuffled( batch_size = 16 ):\n",
    "    print( batch )\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3800f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ed34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (Path( 'output' ) / 'noise2' / 'noise2-000000..tar.gz').as_posix()\n",
    "ds = atdata.Dataset[ts.ImageSample]( url )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e9f2f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<atdata.dataset.SampleBatch object at 0x1125464b0>\n"
     ]
    }
   ],
   "source": [
    "batch: atdata.SampleBatch[ts.SliceRecordingFrame]\n",
    "for batch in ds.shuffled( batch_size = 16 ):\n",
    "    print( batch )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd3aaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.test.forecastbio.cloud/noise3/noise3-000000.tar.gz'\n",
    "ds = atdata.Dataset[ts.ImageSample]( url )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ae776f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<atdata.dataset.SampleBatch object at 0x1125398b0>\n"
     ]
    }
   ],
   "source": [
    "batch: atdata.SampleBatch[ts.ImageSample]\n",
    "for batch in ds.shuffled( batch_size = 16 ):\n",
    "    print( batch )\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toile",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
